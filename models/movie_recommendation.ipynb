{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aadb84bb-9ac3-4bcc-91de-c42847a8b597",
   "metadata": {},
   "source": [
    "# Movie Ratings Matrix Factorization (Collaborative Filtering)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922e83e9-d208-46ff-b955-934847f1ecb3",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fa89d59-cee9-48ff-a958-307bab0b4610",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016ca127-97a9-4b9e-9693-e55447c865f9",
   "metadata": {},
   "source": [
    "## Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa1f26ce-f197-4e4b-8c30-b5389cd06335",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/spark-3.2.1/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2022-05-21 23:00:37,566 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Change the number of cores in this code block\n",
    "# by setting `spark.master` to `local[n]` where\n",
    "# n is the number of cores\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "conf = pyspark.SparkConf().setAll([('spark.master', 'local[4]'),\n",
    "                                   ('spark.app.name', 'MatrixFactorization'),\n",
    "                                   ('spark.memory.offHeap.enabled', True),\n",
    "                                   ('spark.memory.offHeap.size','4g'),\n",
    "                                   ('spark.executor.memory', '4g'), \n",
    "                                   ('spark.driver.memory','6g')])\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef3ade65-8682-49d2-95cf-16b781ae5683",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878ab34c-05cc-4517-af5a-176880f60bcb",
   "metadata": {},
   "source": [
    "## Load final ratings files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9acda688-d8e8-4b82-a1e5-b6480ab8a383",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:==============>                                            (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- Action: double (nullable = true)\n",
      " |-- Adventure: double (nullable = true)\n",
      " |-- Animation: double (nullable = true)\n",
      " |-- Children: double (nullable = true)\n",
      " |-- Comedy: double (nullable = true)\n",
      " |-- Crime: double (nullable = true)\n",
      " |-- Documentary: double (nullable = true)\n",
      " |-- Drama: double (nullable = true)\n",
      " |-- Fantasy: double (nullable = true)\n",
      " |-- Film-Noir: double (nullable = true)\n",
      " |-- Horror: double (nullable = true)\n",
      " |-- Musical: double (nullable = true)\n",
      " |-- Mystery: double (nullable = true)\n",
      " |-- Romance: double (nullable = true)\n",
      " |-- Sci-Fi: double (nullable = true)\n",
      " |-- Thriller: double (nullable = true)\n",
      " |-- War: double (nullable = true)\n",
      " |-- Western: double (nullable = true)\n",
      " |-- avg_rating: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ratings_df = spark.read.csv(\"file:///home/work/data/ratings_100_max.csv\", inferSchema=True, header=True).repartition(100)\n",
    "ratings_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bc5e86a-932d-48ac-b8e8-39b2e19f5e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test = ratings_df.randomSplit([0.8, 0.2], seed=0)\n",
    "train, test = ratings_df.randomSplit([0.5, 0.5], seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "709a8de1-33a0-40c6-8dee-5ee0bcd02a10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[userId: int, movieId: int, rating: double, year: int, Action: double, Adventure: double, Animation: double, Children: double, Comedy: double, Crime: double, Documentary: double, Drama: double, Fantasy: double, Film-Noir: double, Horror: double, Musical: double, Mystery: double, Romance: double, Sci-Fi: double, Thriller: double, War: double, Western: double, avg_rating: double]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking number of partitions \n",
    "# train.rdd.getNumPartitions()\n",
    "ratings_df.unpersist()\n",
    "train.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e909bf6d-77d6-40cd-8f8c-6b8445a07b85",
   "metadata": {},
   "source": [
    "## Building ALS model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17447ea-5ccf-4968-90d8-9ac215587ced",
   "metadata": {},
   "source": [
    "### Alternating Least Squares (ALS) matrix factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "daf8117a-9fbd-442c-90a7-407c677e112e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alternating Least Squares (ALS) matrix factorization\n",
    "from pyspark.ml.recommendation import ALS\n",
    "als = ALS(userCol='userId',\n",
    "          itemCol='movieId',\n",
    "          ratingCol='rating',\n",
    "          nonnegative=True, #setting this to true since we are using ratings > 0.\n",
    "          implicitPrefs=False, #setting this to false as we are using explicit ratings.\n",
    "          coldStartStrategy='drop' # to make sure we don't get NaN evaluation metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66be9f7-8e3f-4f3d-98cf-75c8b51994af",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9782f34-51a6-4443-8b2d-145c07f47bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "param_grid = ParamGridBuilder() \\\n",
    "                .addGrid(als.rank,[10, 50, 100, 150]) \\\n",
    "                .addGrid(als.regParam,[.01, .05, .1, .15]) \\\n",
    "                .build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe51edf-4b6e-4a7d-a94c-d17947e3fc53",
   "metadata": {},
   "source": [
    "The above will generate 4 x 4 = 16 models for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dac7d835-47d7-4e7c-ad4d-05b39df08f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(metricName='rmse',\n",
    "                                labelCol='rating',\n",
    "                                predictionCol='prediction')                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43266aa3-71a5-401a-b732-1a6084b47f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.tuning import CrossValidator\n",
    "# cv = CrossValidator(estimator=als,\n",
    "#                     estimatorParamMaps=param_grid,\n",
    "#                     evaluator=evaluator,\n",
    "#                     numFolds=10)\n",
    "# cv.fit(test)\n",
    "from pyspark.ml.tuning import TrainValidationSplit\n",
    "tvs = TrainValidationSplit(estimator=als, estimatorParamMaps=param_grid, evaluator=evaluator, parallelism=1, seed=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411b45cd-6b0c-4963-ac2e-1db8effab750",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b6e78b4-d754-487a-af39-f73945c3de21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2010:============================================>          (8 + 2) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.62 s, sys: 360 ms, total: 1.98 s\n",
      "Wall time: 1h 23min 46s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tvs_model = tvs.fit(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a164b78a-661d-410d-b929-b46a0917b255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ALSModel: uid=ALS_2055d90fade1, rank=100"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tvs_model.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8658bb54-2207-4dc0-acc3-c3045a3a5eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2174:============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.5 ms, sys: 6.99 ms, total: 16.5 ms\n",
      "Wall time: 28.7 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6888509596295516"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "evaluator.evaluate(tvs_model.transform(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e67e48d-e574-4cfb-a533-ef0e1eda4d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8984551544862247"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(tvs_model.transform(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ac27105-66fc-43f5-a4f8-7e0d7093a3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/work/dse230_project/models\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ee8a1bb-4163-445c-95bb-2b526aa1203c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "write_files = True\n",
    "if write_files:\n",
    "    tvs_model.save(\"file:///home/work/data/als_model_test\")\n",
    "    # !hadoop fs -ls /tsv_model\n",
    "    # !hadoop fs -copyToLocal /tsv_model/* /home/work/data/als_model_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c40b529-7b3d-48a2-9122-f7e2c410b06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/tsv_model': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -rm -r /tsv_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b15d3aa-440c-4d26-bb30-7127fc778b14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "741d87a7-56a9-4b1d-8348-c2e377d1a97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fbf645-bc3c-47e3-881b-13c635f26ae8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": true,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
